{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'data_original.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "FOLD_COUNT = 10\n",
    "list_classes = ['不受理', '不成立', '成立', '當事人不到場', '聲請人撤回']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data_original.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-64789c5e434b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data_original.csv' does not exist"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments = []\n",
    "temp = train_df['調解內容與決議'].fillna('no sentence').values\n",
    "for line in temp:\n",
    "    if '經調解結果如左：' in line:\n",
    "        a = line.split('經調解結果如左：')[0]\n",
    "    else:\n",
    "        a = line.split('經調解結果如下：')[0]\n",
    "    cleaned_comments.append(a)\n",
    "train_df['調解內容與決議'] = cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(train_df.isnull()) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['調解內容與決議'].fillna('no comment').values\n",
    "train_comments_lengths = [len(str(s)) for s in train_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['是否成立'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pd.Series(train_comments_lengths).astype(int).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Lable to One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Score = train_df['是否成立']\n",
    "data = pd.get_dummies(Score)\n",
    "train_df = pd.concat([train_df, data], axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df.iloc[:, 22:].sum()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Summary\")\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Results', fontsize=12)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, ha='center', va='bottom', s='{:.1f}'.format(abs(label)))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.plasma\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Correlation of features', y=1.05, size=14)\n",
    "sns.heatmap(data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(train_df['people'].fillna('no people').values)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = train_df.apply(lambda row: row['是否成立'] == '成立' or row['是否成立'] == '不成立', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = train_df[new_df]\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "people, ty, success, unsuc= [], [], [], []\n",
    "for n in pd.unique(df_new['people'].values):\n",
    "    for t in pd.unique(df_new['案件細節類型'].values):\n",
    "        a = df_new[df_new['people'] == n]\n",
    "        if a[a['案件細節類型'] == t]['是否成立'].empty:\n",
    "            continue\n",
    "        else:\n",
    "            b = a[a['案件細節類型'] == t]['是否成立'].value_counts()\n",
    "            people.append(n)\n",
    "            ty.append(t)\n",
    "            if len(b) == 2:\n",
    "                success.append(b['成立'])\n",
    "                unsuc.append(b['不成立'])\n",
    "            else:\n",
    "                try:\n",
    "                    success.append(b['成立'])\n",
    "                    unsuc.append('0')\n",
    "                except:\n",
    "                    success.append('0')\n",
    "                    unsuc.append(b['不成立'])\n",
    "print(len(people), len(ty), len(success), len(unsuc))\n",
    "result['people'] = people\n",
    "result['案件細節類型'] = ty\n",
    "result['成立'] = success\n",
    "result['不成立'] = unsuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('results/success.csv', encoding='big5', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Clearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=False):\n",
    "    # remove url\n",
    "    text = re.sub(r\"(https?:\\/\\/)*(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    \n",
    "    # Special expressions\n",
    "    text = re.sub(r'〈下同〉', '', text)\n",
    "    text = re.sub(r'（車號：[a-zA-Z0-9－]*號）', '', text)\n",
    "    text = re.sub(r'（[\\' \\']*）', '', text)\n",
    "    text = re.sub(r'（下同）', '', text)\n",
    "    text = re.sub(r'\\\\r\\\\n[0-9a-zA-Z\\\\r\\\\n]*', '', text)\n",
    "    text = re.sub(r'(口)', '', text)\n",
    "    \n",
    "    text = re.sub(r',', '，', text)\n",
    "    text = re.sub(r'\\.+', '...', text)\n",
    "    text = re.sub(r'\\.{6}', '...', text)\n",
    "    text = re.sub(r'…', '...', text)\n",
    "    text = re.sub(r';', '；', text)\n",
    "    text = re.sub(r'°', '。', text)\n",
    "    text = re.sub(r'】', ']', text)\n",
    "    text = re.sub(r'【', '[', text)\n",
    "    text = re.sub(r'\\)', '\\）', text)\n",
    "    text = re.sub(r'\\(', '\\（', text)\n",
    "    text = re.sub(r'“', '\"', text)\n",
    "    text = re.sub(r' ', '', text)\n",
    "    text = re.sub(r'”', '\"', text)\n",
    "    text = re.sub(r'～', '~', text)\n",
    "    text = re.sub(r'·', '。', text)\n",
    "    text = re.sub(r'!', '！', text)\n",
    "    text = re.sub(r'—', '-', text)\n",
    "    text = re.sub(r'》', '\\）', text)\n",
    "    text = re.sub(r'《', '\\（', text)\n",
    "    text = re.sub(r'\\?', '\\？', text)\n",
    "    text = re.sub(r'。。。', '...', text)\n",
    "    text = re.sub(r'。。。。。。', '...', text)\n",
    "    text = re.sub(r':', '：', text)\n",
    "    \n",
    "    text = special_alpha_removal.sub('', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to remove all Alpha Numeric and space\n",
    "special_alpha_removal = re.compile(r'[a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numeric\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "# regex to replace ###...\n",
    "replace_sharp = re.compile(r'[#]+', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_comments = []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in train_comments:\n",
    "    cleaned_train_comments.append(clean_text(text))\n",
    "    \n",
    "train_df['cleaned_comments'] = cleaned_train_comments\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word segmentation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = defaultdict(int)\n",
    "\n",
    "for sentence in tqdm(train_df['調解內容與決議']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    for word in seg_list:\n",
    "        word_dict[word] += 1\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_word_dict = defaultdict(int)\n",
    "\n",
    "for sentence in tqdm(train_df['cleaned_comments']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    for word in seg_list:\n",
    "        cleaned_word_dict[word] += 1\n",
    "cleaned_word_dict = sorted(cleaned_word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(cleaned_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentences = []\n",
    "for sentence in tqdm(train_df['cleaned_comments']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    cut_sentences.append(\" \".join(seg_list))\n",
    "train_df['cleaned_comments_cut'] = cut_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_train_sentences = train_df['cleaned_comments_cut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_train_sentences = train_df['cleaned_comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick view of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list, sign_list, dig_english_list = [], [], []\n",
    "for word, count in word_dict:\n",
    "    for char in word:\n",
    "        if char >= u'\\u4E00' and char <= u\"\\u9FA5\":\n",
    "            chinese_list.append((word, count))\n",
    "        elif (char >= u'\\u0041' and char <= u'\\u005A') or (char >= u'\\u0061' and char <= u'\\u007A') or (char >= u'\\u0030' and char <= u'\\u0039'):\n",
    "            dig_english_list.append((word, count))\n",
    "            break\n",
    "        else:\n",
    "            sign_list.append((word, count))\n",
    "            break\n",
    "sorted_dig_english_list = sorted(set(dig_english_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_sign_list = sorted(set(sign_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_chinese_list = sorted(set(chinese_list), key=lambda x: x[1], reverse=True)\n",
    "print(\"chinese_word: \", len(sorted_chinese_list))\n",
    "print(\"dig_english_word: \", len(sorted_dig_english_list))\n",
    "print(\"sign_count: \", len(sorted_sign_list))\n",
    "print(sorted_chinese_list[:10000], '\\n\\n', sorted_dig_english_list[:50], '\\n\\n', sorted_sign_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list, sign_list, dig_english_list = [], [], []\n",
    "for word, count in cleaned_word_dict:\n",
    "    for char in word:\n",
    "        if char >= u'\\u4E00' and char <= u\"\\u9FA5\":\n",
    "            chinese_list.append((word, count))\n",
    "        elif (char >= u'\\u0041' and char <= u'\\u005A') or (char >= u'\\u0061' and char <= u'\\u007A') or (char >= u'\\u0030' and char <= u'\\u0039'):\n",
    "            dig_english_list.append((word, count))\n",
    "            break\n",
    "        else:\n",
    "            sign_list.append((word, count))\n",
    "            break\n",
    "sorted_dig_english_list = sorted(set(dig_english_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_sign_list = sorted(set(sign_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_chinese_list = sorted(set(chinese_list), key=lambda x: x[1], reverse=True)\n",
    "print(\"chinese_word: \", len(sorted_chinese_list))\n",
    "print(\"dig_english_word: \", len(sorted_dig_english_list))\n",
    "print(\"sign_count: \", len(sorted_sign_list))\n",
    "print(sorted_chinese_list[:10000], '\\n\\n', sorted_dig_english_list[:50], '\\n\\n', sorted_sign_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cut_train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cut_train_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=200000\n",
    ")\n",
    "word_vectorizer.fit(cut_train_sentences)\n",
    "train_word_features = word_vectorizer.transform(cut_train_sentences)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features = train_word_features.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Veiw & Processing Col Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = np.array([cols[3]] + cols[5:13] + cols[14:21])\n",
    "label_column = np.array([cols[13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['結案時間'] = [replace_sharp.sub('99:99', str(text)) for text in train_df['結案時間'].fillna(' ').values]\n",
    "train_df['收件時間'] = [replace_sharp.sub('99:99', str(text)) for text in train_df['收件時間'].fillna(' ').values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = []\n",
    "for lin in train_df['對照人'].fillna(' ').values:\n",
    "    temp = \"000000\"\n",
    "    if temp in str(lin):\n",
    "        new_col.append(lin)\n",
    "    else:\n",
    "        new_col.append(\"gb\")\n",
    "\n",
    "print(len(new_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = []\n",
    "for column in data_columns:\n",
    "    try:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[column].fillna(' ').values))\n",
    "        feature_data.append(le.transform(list(train_df[column].fillna(' ').values)))\n",
    "    except:\n",
    "        print(column)\n",
    "feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_sets = []\n",
    "for line in zip(feature_data[0], feature_data[0], feature_data[1], feature_data[2], feature_data[3], feature_data[4], feature_data[5], \n",
    "                feature_data[6], feature_data[7], feature_data[8], feature_data[9], feature_data[10], feature_data[11], feature_data[12],\n",
    "                feature_data[13], feature_data[14], feature_data[15]):\n",
    "    train_feature_sets.append(line)\n",
    "train_feature_sets = np.array(train_feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model with Tfidf Features (Multi-Lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        predictions[class_name] = classifier.predict_proba(train_tfidf_features)[:, 1]\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_feature_sets)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, \n",
    "                   max_iter=100, multi_class='ovr', penalty='l2', random_state=None, solver='liblinear', tol=0.0001,verbose=0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_feature_sets[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_feature_sets[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_feature_sets[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        predictions[class_name] = classifier.predict_proba(train_feature_sets)[:, 1]\n",
    "        \n",
    "    labels_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOB (Out-of-Bag) Evaluation (Error ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "\n",
    "for i, model in enumerate(tfidf_models):\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(train_tfidf_features)[:, 1]\n",
    "    \n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "for i, model in enumerate(labels_models):\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(train_feature_sets)[:, 1]\n",
    "        \n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = ['results/Submission_file_{}.csv'.format(i) for i in range(10, FOLD_COUNT * 2)]\n",
    "bagging(result_list, 'results/bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('results/bagging.csv', encoding='big5')\n",
    "result_label = test_df[list_classes]\n",
    "results = result_label.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target = train_df['是否成立']\n",
    "print('Validation accuracy is {}'.format(accuracy_score(results, val_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df['是否成立']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "multi_classifier_tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features, train_label)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    train_target = train_label[train_idx]\n",
    "    classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "    y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "\n",
    "    val_target = train_label[test_idx]\n",
    "    val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "\n",
    "    print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    multi_classifier_tfidf_models.append(classifier)\n",
    "    predictions = classifier.predict_proba(train_tfidf_features)[:, 1]\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/multi/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_feature_sets, train_label)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, \n",
    "                   max_iter=100, multi_class='ovr', penalty='l2', random_state=None, solver='liblinear', tol=0.0001,verbose=0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    train_target = train_label[train_idx]\n",
    "\n",
    "    classifier.fit(train_feature_sets[train_idx], train_target)\n",
    "    y_pred = classifier.predict(train_feature_sets[train_idx])\n",
    "\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "\n",
    "    val_target = train_label[test_idx]\n",
    "    val_pred = classifier.predict(train_feature_sets[test_idx])\n",
    "\n",
    "    print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    multi_classifier_tfidf_models.append(classifier)\n",
    "    predictions[class_name] = classifier.predict_proba(train_feature_sets)[:, 1]\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/multi/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/LR_Based/ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr, encoding='big5'))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False, encoding='big5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
