{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import lightgbm as lgb\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from keras import optimizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING = 'pretrain_embedding/glove.840B.300d.txt'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "FOLD_COUNT = 10\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is word, value is pretrained word embedding.\n",
    "    \"\"\"\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_words(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is typo, value is correct word.\n",
    "    \"\"\"\n",
    "    clean_word_dict = {}\n",
    "    with open(file, 'r', encoding='utf-8'):\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            clean_word_dict[typo] = correct\n",
    "    return clean_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_index = load_pretrain_embedding(GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in list_classes:\n",
    "    print('{}\\n{}\\n'.format(class_name, train_df[class_name].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = load_clean_words(CLEAN_WORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = sorted([f for f in listdir('datasets/feature_files/') if isfile(join('datasets/feature_files/', f))])\n",
    "for file in feature_files:\n",
    "    fixed_df = pd.read_csv('datasets/feature_files/' + file)\n",
    "    train_df = train_df.merge(fixed_df, on='id', how='left')\n",
    "    test_df = test_df.merge(fixed_df, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_data(path):\n",
    "    files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "    for i, file in enumerate(files):\n",
    "        temp_df = pd.read_csv(path + file)\n",
    "        print('Datasets before ensemble null number:', temp_df.isnull().sum().sum())\n",
    "        if i == 0:\n",
    "            ensembled = temp_df\n",
    "        else:\n",
    "            ensembled = ensembled.merge(temp_df, on='id', how='left')\n",
    "            print('Datasets after ensemble null number:', ensembled.isnull().sum().sum())\n",
    "    \n",
    "    column_numbers = ensembled.shape[1]\n",
    "    toxic = ensembled.iloc[:, [i for i in range(2, column_numbers, len(list_classes))]]\n",
    "    severe_toxic = ensembled.iloc[:, [i for i in range(3, column_numbers, len(list_classes))]]\n",
    "    obscene = ensembled.iloc[:, [i for i in range(4, column_numbers, len(list_classes))]]\n",
    "    threat = ensembled.iloc[:, [i for i in range(5, column_numbers, len(list_classes))]]\n",
    "    insult = ensembled.iloc[:, [i for i in range(6, column_numbers, len(list_classes))]]\n",
    "    identity_hate = ensembled.iloc[:, [i for i in range(7, column_numbers, len(list_classes))]]\n",
    "    \n",
    "    return toxic, severe_toxic, obscene, threat, insult, identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate = ensemble_data('datasets/ensemble_files/train/')\n",
    "test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate = ensemble_data('datasets/ensemble_files/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        text = re.sub(r\"\\.jpg\", \" \", text)\n",
    "        text = re.sub(r\"\\.png\", \" \", text)\n",
    "        text = re.sub(r\"\\.gif\", \" \", text)\n",
    "        text = re.sub(r\"\\.bmp\", \" \", text)\n",
    "        text = re.sub(r\"\\.pdf\", \" \", text)\n",
    "        text = re.sub(r\"image:\", \" \", text)\n",
    "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
    "        \n",
    "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
    "        \n",
    "        text = re.sub(r\"wp:\", \" \", text)\n",
    "        text = re.sub(r\"file:\", \" \", text)\n",
    "    \n",
    "    # For Punctuation\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = re.sub(r\"´\", \"'\", text)\n",
    "    text = re.sub(r\"—\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"−\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"#\", \" # \", text)\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"<3\", \" love \", text)\n",
    "        \n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        for typo, correct in ignored_words.items():\n",
    "            text = re.sub(typo, correct, text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta (Extend) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unknown_embedding(t, idx):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for word in t:\n",
    "        if word not in idx.keys():\n",
    "            res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regexp_occ(regexp='', text=None):\n",
    "    \"\"\"\n",
    "    Simple way to calculate the number of occurence of a regex\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_feature(df):\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capital'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capital']) / float(row['total_length']), axis=1)\n",
    "    df[\"unknown_glove\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, glove_embeddings_index))\n",
    "    df[\"unknown_fasttext\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, embeddings_index))\n",
    "    df[\"unknown_glove_fasttext\"] = df[\"unknown_glove\"] + df[\"unknown_fasttext\"]\n",
    "    # Special Expressions Collection\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    df[\"has_star\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\*\", x))\n",
    "    ## Check for dates\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))    # example: 18:44, 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    ## Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    df[\"has_ip\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", x))\n",
    "    ## Check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x))\n",
    "    ## Check for image\n",
    "    df[\"has_image\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'image\\:', x))\n",
    "    # Dirty Languages Collection\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating meta features...\")\n",
    "create_meta_feature(train_df)\n",
    "create_meta_feature(test_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train_df.columns[9:]\n",
    "print(train_cols)\n",
    "column_numbers = len(train_df.columns)\n",
    "print(column_numbers, len(train_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_cols:\n",
    "    train_toxic[col] = train_df[col]\n",
    "    train_severe_toxic[col] = train_df[col]\n",
    "    train_obscene[col] = train_df[col]\n",
    "    train_threat[col] = train_df[col]\n",
    "    train_insult[col] = train_df[col]\n",
    "    train_identity_hate[col] = train_df[col]\n",
    "for col in train_cols:\n",
    "    test_toxic[col] = test_df[col]\n",
    "    test_severe_toxic[col] = test_df[col]\n",
    "    test_obscene[col] = test_df[col]\n",
    "    test_threat[col] = test_df[col]\n",
    "    test_insult[col] = test_df[col]\n",
    "    test_identity_hate[col] = test_df[col]\n",
    "train_ensembled_data = [train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate]\n",
    "test_ensembled_data = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_sentences_train, list_sentences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vect = TfidfVectorizer(max_features=5000, stop_words=ignored_words)\n",
    "word_vect.fit(all_comment_text)\n",
    "data = word_vect.fit_transform(all_comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(all_comment_text)\n",
    "train_word_features = word_vectorizer.transform(cleaned_train_comments)\n",
    "test_word_features = word_vectorizer.transform(cleaned_test_comments)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000\n",
    ")\n",
    "char_vectorizer.fit(all_comment_text)\n",
    "train_char_features = char_vectorizer.transform(cleaned_train_comments)\n",
    "test_char_features = char_vectorizer.transform(cleaned_test_comments)\n",
    "print('Char vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_comment_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_tfidf_features = hstack([test_char_features, test_word_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_char_features, train_word_features, test_char_features, test_word_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=True, lower=False)\n",
    "\n",
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in tqdm(train_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in tqdm(test_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_comment_text'] = cleaned_train_comments\n",
    "test_df['cleaned_comment_text'] = cleaned_test_comments\n",
    "train_df.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "test_df.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cleaned_train_comments, cleaned_test_comments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Pytorch Variable (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches_generator(inputs, targets, batch_size, row_shuffle=False):\n",
    "    inputs_data_size = len(inputs)\n",
    "    targets_data_size = len(targets)\n",
    "    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent.\".format(inputs_data_size, targets_data_size)\n",
    "    \n",
    "    if row_shuffle:\n",
    "        for input_seqs in inputs:\n",
    "            np.random.shuffle(input_seqs)\n",
    "            \n",
    "    shuffled_input, shuffled_target = shuffle(inputs, targets)\n",
    "    mini_batches = [\n",
    "        (shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n",
    "        for k in range(0, inputs_data_size, batch_size)\n",
    "    ]\n",
    "    dp = EmbeddingDropout(p=0.2)\n",
    "    \n",
    "    for batch_xs, batch_ys in mini_batches:\n",
    "        lengths = [len(s) for s in batch_xs]\n",
    "        max_length = min(MAX_SEQUENCE_LENGTH, max(lengths))\n",
    "        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding='post', truncating='pre')\n",
    "        \n",
    "        lengths_var = Variable(torch.Tensor(lengths), requires_grad=False)\n",
    "        inputs_tensor = torch.from_numpy(batch_tensors).long()\n",
    "        inputs_dropped_tensor = dp.forward(inputs_tensor)\n",
    "        inputs_var = Variable(inputs_dropped_tensor)\n",
    "        targets_var = Variable(torch.from_numpy(batch_ys).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_var = inputs_var.cuda()\n",
    "            targets_var = targets_var.cuda()\n",
    "            lengths_var = lengths_var.cuda()\n",
    "        yield (inputs_var, lengths_var), targets_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batches_generator(inputs, batch_size):\n",
    "    inputs_data_size = len(inputs)\n",
    "    mini_batches = [\n",
    "        inputs[k: k + batch_size]\n",
    "        for k in range(0, inputs_data_size, batch_size)\n",
    "    ]\n",
    "    \n",
    "    for batch_xs in mini_batches:\n",
    "        lengths = [len(s) for s in batch_xs]\n",
    "        max_length = min(MAX_SEQUENCE_LENGTH, max(lengths))\n",
    "        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding='post', truncating='pre')\n",
    "        \n",
    "        lengths_var = Variable(torch.Tensor(lengths))\n",
    "        inputs_var = Variable(torch.from_numpy(batch_tensors).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_var = inputs_var.cuda()\n",
    "            lengths_var = lengths_var.cuda()\n",
    "            \n",
    "        yield (inputs_var, lengths_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding (Build a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open(PATH + 'null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorting null_words...')\n",
    "null_dict = {}\n",
    "with open(PATH + 'null_words.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        word, count = line.strip('\\n').split(', ')\n",
    "        null_dict[word] = int(count)\n",
    "\n",
    "null_dict = sorted(null_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open(PATH + 'null_words.txt', 'w', encoding='utf-8') as output:\n",
    "    for word, count in null_dict:\n",
    "        output.write(word + ', ' + str(count) + '\\n')\n",
    "print('Sorting operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with Using Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    # predictions = {'id': test_df['id']}\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = ['results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i) for i in range(0, FOLD_COUNT)]\n",
    "bagging(result_list, 'results/TFIDF_Based/TFIDF_bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "# et_predictions = {'id': test_df['id']}\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    print(classifier.feature_importances_)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/TFIDF_Based/TFIDF_ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light-GBM For Ensembled_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_every_feature_model(feature_data, label, feature_name, feature_test_data, fold_count, predict=False):\n",
    "    predictions = np.zeros(shape=(len(feature_test_data)))\n",
    "    fold_size = len(feature_data) // fold_count\n",
    "    \n",
    "    print('Feature name is {}'.format(feature_name))\n",
    "    auc_score = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        print('## Fold In {} ##'.format(fold_id + 1))\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(feature_data)\n",
    "            \n",
    "        train_x = np.concatenate((feature_data[:fold_start], feature_data[fold_end:]))\n",
    "        train_y = np.concatenate((label[:fold_start], label[fold_end:]))\n",
    "        \n",
    "        val_x = feature_data[fold_start:fold_end]\n",
    "        val_y = label[fold_start:fold_end]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_val = lgb.Dataset(val_x, val_y)\n",
    "        \n",
    "        lgbm_model = lgb.LGBMClassifier(max_depth=5, metric='auc', n_estimators=10000, num_leaves=32, boosting_type='gbdt', \\\n",
    "                                       learning_rate=0.01, feature_fraction=0.3, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0)\n",
    "        lgbm_model.fit(X=train_x, y=train_y, eval_metric=['auc', 'binary_logloss'], eval_set=(val_x, val_y), early_stopping_rounds=1000, verbose=500)\n",
    "        auc_score += lgbm_model.best_score_['valid_0']['auc']\n",
    "        lgb.plot_importance(lgbm_model, max_num_features=30)\n",
    "        plt.show()\n",
    "        if predict:\n",
    "            prediction = lgbm_model.predict_proba(feature_test_data)[:, 1]\n",
    "            predictions += prediction\n",
    "            del lgbm_model\n",
    "    predictions /= fold_count\n",
    "    print('Training LightGBM Done!')\n",
    "    return predictions, auc_score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc_scores = [], []\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    prediction, auc = fit_every_feature_model(train_ensembled_data[i], train_df[feature_name].values, feature_name, test_ensembled_data[i], 10, predict=True)\n",
    "    auc_scores.append(auc)\n",
    "    predictions.append(prediction)\n",
    "print('Overall AUC Score is {}'.format(sum(auc_scores) / 6))\n",
    "print('For each:'.format(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('datasets/sample_submission.csv')\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    submission[feature_name] = predictions[i]\n",
    "submission.to_csv('results/LightGBM/LightGBM_with_Meta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'model_pool/Keras/av_pos_cnn/pavel_cnn_%.2f_%.2f'%(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weight = None\n",
    "    best_epoch = 0\n",
    "    current_epoch = 1\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epoch=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print('Epoch {} auc {:.6f} best_auc {:.6f}'.format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weight = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            # early stop\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "                \n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    best_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot': train_x, 'POS': pos_train_x}\n",
    "    val_data = {'Onehot': val_x, 'POS': pos_val_x}\n",
    "    \n",
    "    hist = model.fit(train_data, train_y, validation_data=(val_data, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    best_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print('AUC Score', auc)\n",
    "    return model, best_val_score, auc, predictions\n",
    "\n",
    "def train_folds(x, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate((pos_x[:fold_start], pos_x[fold_end:]))\n",
    "        pos_val_x = pos_x[fold_start: fold_end]\n",
    "        print('## In fold {} ## : '.format(fold_id + 1))\n",
    "        model, best_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_log_loss(y, y_pred):\n",
    "    total_loss = 0\n",
    "    for j in range(6):\n",
    "        loss = log_loss(y[:, j], y_pred[:, j])\n",
    "        total_loss += loss\n",
    "    total_loss /= 6.\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "epoch_num = 100\n",
    "early_stop_round = 12\n",
    "MODELSTAMP = 'model_pool/Pytorch/rhn/pavel_rhn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_decay(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.93\n",
    "    return optimizer\n",
    "\n",
    "def _train_batch(model, inputs_var, targets_var, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    preds_var = model.forward(inputs_var)\n",
    "    \n",
    "    # Pytorch version requires torch.cuda.FloatTensor rather than torch.cuda.LongTensor\n",
    "    preds_var = preds_var.type(torch.cuda.FloatTensor)\n",
    "    targets_var = targets_var.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    # training\n",
    "    loss = F.binary_cross_entropy_with_logits(preds_var, targets_var)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "    optimizer.step()\n",
    "    return optimizer, loss.data.cpu().numpy()\n",
    "\n",
    "def _pytorch_train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n",
    "    '''\n",
    "    Train for 6-labels at the same time.\n",
    "    '''\n",
    "    print(\"## Training on fold {} ##\".format(fold_id))\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    best_auc, best_logloss, best_epoch, current_epoch = -1, -1, 0, 1\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_logloss = 0.0\n",
    "        for batch_id, (inputs_var, targets_var) in enumerate(mini_batches_generator(train_x, train_y, batch_size, row_shuffle=True)):\n",
    "            optimizer, loss = _train_batch(model=model, inputs_var=inputs_var, targets_var=targets_var, optimizer=optimizer)\n",
    "            \n",
    "            epoch_logloss += loss\n",
    "            if batch_id % 40 == 0:\n",
    "                print(\"Epoch: {} Batch: {} Log-loss: {}\".format(epoch + 1, batch_id, loss))\n",
    "                \n",
    "        print(\"Epoch average log-loss: {}\".format(epoch_logloss / batch_id))\n",
    "        val_pred = model.predict(val_x)\n",
    "        \n",
    "        current_logloss = self_log_loss(val_y, val_pred)\n",
    "        current_epoch += 1\n",
    "        if best_logloss > current_logloss or best_logloss == -1:\n",
    "            best_logloss = current_logloss\n",
    "            model.save(MODELSTAMP + '-TEMP.pt')\n",
    "            best_auc = roc_auc_score(val_y, val_pred)\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == early_stop_round:\n",
    "                break\n",
    "        print(\"In Epoch: {}, val_loss: {}, best_val_loss: {}, best_auc: {}\".format(epoch + 1, current_logloss, best_logloss, best_auc))\n",
    "        optimizer = learning_rate_decay(optimizer)\n",
    "        \n",
    "    model.load(MODELSTAMP + '-TEMP.pt')\n",
    "    best_val_pred = model.predict(val_x)\n",
    "    model.save(MODELSTAMP + str(fold_id) + '.pt')\n",
    "    return model, best_logloss, best_auc, best_val_pred\n",
    "\n",
    "def pytorch_train_folds(x, y, fold_count, batch_size, get_model_func, skip_fold=0):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        if fold_id < skip_fold:\n",
    "            model = get_model_func()\n",
    "            model.load(MODELSTAMP + str(fold_id) + '.pt')\n",
    "            model = model.eval()\n",
    "            model = model.cuda()\n",
    "            fold_prediction = model.predict(val_x)\n",
    "            auc = roc_auc_score(val_y, fold_prediction)\n",
    "            bst_val_score = self_log_loss(y=val_y, y_pred=fold_prediction)\n",
    "        \n",
    "        else:\n",
    "            model, best_val_score, auc, fold_prediction = _pytorch_train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Model for computing a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter per channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        attn_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(attn_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, attn_weights]\n",
    "        return result\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    With TensorFlow Backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2] * self.k)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # top_k function can only be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, self.k, True, None)[0]\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 300\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_maxpool, merged_attn, merged_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words, \n",
    "                                embedding_dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=max_sequence_length, \n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    30,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    filter_nums = 325\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    \n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences])\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(merged_embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_tensor_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_tensor_maxpool, merged_tensor_attn, merged_tensor_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 180\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    filter_nums = 128\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    conv_layer = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu', strides=1)(rnn_layer)\n",
    "    \n",
    "    maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    attn = AttentionWeightedAverage()(conv_layer)\n",
    "    avg = GlobalAveragePooling1D()(conv_layer)\n",
    "    \n",
    "    merged_tensor = merge([maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=120, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    merged_rnn_layer = merge([rnn_layer_0, rnn_layer_1], mode='concat', concat_axis=2)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(merged_rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(merged_rnn_layer)\n",
    "    attn = AttentionWeightedAverage()(merged_rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(merged_rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    35,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences], axis=2)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(merged_embedding_layer)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_0 = SpatialDropout1D(0.3)(rnn_layer_0)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(rnn_layer_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_layer_1)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer_1)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_bigru(nb_words, embedding_dims, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Emebdding(nb_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer = Dropout(0.35)(rnn_layer)\n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1])(rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=72, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_vector = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "        \n",
    "        init.xavier_uniform(self.attn_vector.data)\n",
    "        \n",
    "    def forward(self, inputs, lengths=None):\n",
    "        \"\"\"\n",
    "        Return a scalar (All hiddens to only one weight for one output)\n",
    "        (batch_size, max_len, hidden_size) * (batch_size, hidden_size, 1) --> (batch_size, max_len, 1)\n",
    "        \"\"\"\n",
    "        batch_size, max_len = inputs.size()[:2]\n",
    "        \n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.attn_vector              # (1, hidden)\n",
    "                            .unsqueeze(0)                 # (1, 1, hidden)\n",
    "                            .transpose(2, 1)              # (1, hidden, 1)\n",
    "                            .repeat(batch_size, 1, 1))    # (batch_size, hidden, 1)\n",
    "        \n",
    "        attn_energies = F.softmax(F.relu(weights.squeeze()))\n",
    "        \n",
    "        # create mask based on the sentence length\n",
    "        mask = Variable(torch.ones(attn_energies.size())).cuda()\n",
    "        for i, l in enumerate(lengths):\n",
    "            if l < max_len:\n",
    "                mask[:, l:] = 0\n",
    "                \n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked = attn_energies * mask\n",
    "        _sums = masked.sum(-1).expand_as(attn_energies)\n",
    "        attention_weights = masked.div(_sum)\n",
    "        \n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attention_weights.unsqueeze(-1).expand_as(inputs))\n",
    "        \n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        \n",
    "        return representations, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout():\n",
    "    \"\"\"\n",
    "    Implement of word embedding dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.trainable = True\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            dim = inputs.dim()\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(1, -1)\n",
    "            batch_size = inputs.size(0)\n",
    "            for i in range(batch_size):\n",
    "                x = np.unique(inputs[i].numpy())\n",
    "                x = np.nonzero(x)[0]\n",
    "                if len(x) == 0:\n",
    "                    return inputs\n",
    "                x = torch.from_numpy(x)\n",
    "                noise = x.new().resize_as_(x)\n",
    "                noise.bernoulli_(self.p)\n",
    "                x = x.mul(noise)\n",
    "                for value in x:\n",
    "                    if value > 0:\n",
    "                        mask = inputs[i].eq(value)\n",
    "                        inputs[i].masked_fill_(mask, 0)\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(-1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super(SequentialDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.restart = True\n",
    "        self.trainable = True\n",
    "        \n",
    "    def _make_noise(self, inputs):\n",
    "        return Variable(inputs.data.new().resize_as_(inputs.data))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            if self.restart:\n",
    "                self.noise = self._make_noise(inputs)\n",
    "                self.noise.data.bernoulli_(1 - self.p).div_(1 - self.p)\n",
    "                if self.p == 1:\n",
    "                    self.noise.data.fill_(0)\n",
    "                self.noise = self.noise.expand_as(inputs)\n",
    "                self.restart = False\n",
    "            return inputs.mul(self.noise)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def end_of_sequence(self):\n",
    "        self.restart = True\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.end_of_sequence()\n",
    "        if self.p > 0 and self.trainable:\n",
    "            return grad_output.mul(self.noise)\n",
    "        else:\n",
    "            return grad_output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + '({:.4f})'.format(self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Embedding & Sequential Dropout\n",
    "seq_drop_model = SequentialDropout(p=0.5)\n",
    "input_data= Variable(torch.ones(1, 10), volatile=True)\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "output_last = seq_drop_model(input_data)\n",
    "for i in range(50):\n",
    "    output_new = seq_drop_model(input_data)\n",
    "    dist_total += torch.dist(output_new, output_last).data\n",
    "    output_last = output_new\n",
    "    \n",
    "if not torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    print(dist_total)\n",
    "    \n",
    "seq_drop_model.end_of_sequence()\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "for i in range(50):\n",
    "    dist_total += torch.dist(output_last, seq_drop_model(input_data)).data\n",
    "    seq_drop_model.end_of_sequence()\n",
    "    \n",
    "if torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    \n",
    "emb_drop_model = EmbeddingDropout(p=0.15)\n",
    "input_data = torch.Tensor([[1,2,3,0,0], [5,3,2,2,0]]).long()\n",
    "print(input_data)\n",
    "print(emb_drop_model.forward(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGRUCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(AbstractGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias_ih = bias_ih\n",
    "        self.bias_hh = bias_hh\n",
    "        \n",
    "        self.weight_wr = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_wz = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_wh = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_ur = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        self.weight_uz = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        self.weight_uh = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        # Interface\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(AbstractGRUCell):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(GRUCell, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        if hx is None:\n",
    "            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n",
    "        r = F.sigmoid(self.weight_wr(x) + self.weight_ur(hx))\n",
    "        z = F.sigmoid(self.weight_wz(x) + self.weight_uz(hx))\n",
    "        ht = F.tanh(self.weight_wh(x) + self.weight_uh(r * hx))\n",
    "        hx = (1 - i) * hx  + z * ht\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGRUCell(AbstractGRUCell):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False, dropout=0.25):\n",
    "        super(BayesianGRUCell, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        self.dropout = dropout\n",
    "        self.set_dropout(self.dropout)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.drop_wr = SequentialDropout(p=dropout)\n",
    "        self.drop_wz = SequentialDropout(p=dropout)\n",
    "        self.drop_wh = SequentialDropout(p=dropout)\n",
    "        self.drop_ur = SequentialDropout(p=dropout)\n",
    "        self.drop_uz = SequentialDropout(p=dropout)\n",
    "        self.drop_uh = SequentialDropout(p=dropout)\n",
    "    \n",
    "    def end_of_sequence(self):\n",
    "        self.drop_wr.end_of_sequence()\n",
    "        self.drop_wz.end_of_sequence()\n",
    "        self.drop_wh.end_of_sequence()\n",
    "        self.drop_ur.end_of_sequence()\n",
    "        self.drop_uz.end_of_sequence()\n",
    "        self.drop_uh.end_of_sequence()\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        if hx is None:\n",
    "            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n",
    "        x_wr = self.drop_wr(x)\n",
    "        x_wz = self.drop_wz(x)\n",
    "        x_wh = self.drop_wh(x)\n",
    "        x_ur = self.drop_ur(hx)\n",
    "        x_uz = self.drop_uz(hx)\n",
    "        x_uh = self.drop_uh(hx)\n",
    "        r = F.sigmoid(self.weight_wr(x_wr) + self.weight_ur(x_ur))\n",
    "        z = F.sigmoid(self.weight_wz(x_wz) + self.weight_uz(x_uz))\n",
    "        ht = F.tanh(self.weight_wh(x_wh) + self.weight_uh(r * x_uh))\n",
    "        hx = (1 - z) * hx + z * ht\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHNCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, is_first_layer, recurrent_dropout):\n",
    "        super(RHNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_first_layer = is_first_layer\n",
    "        \n",
    "        self.set_dropout(recurrent_dropout)\n",
    "        \n",
    "        if self.is_first_layer:\n",
    "            self.W_H = nn.Linear(input_size, hidden_size)\n",
    "            self.W_C = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.R_H = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.R_C = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = dropout\n",
    "        self.drop_ir = SequentialDropout(p=self.dropout)\n",
    "        self.drop_ii = SequentialDropout(p=self.dropout)\n",
    "        self.drop_hr = SequentialDropout(p=self.dropout)\n",
    "        self.drop_hi = SequentialDropout(p=self.dropout)\n",
    "        \n",
    "    def end_of_sequence(self):\n",
    "        self.drop_ir.end_of_sequence()\n",
    "        self.drop_ii.end_of_sequence()\n",
    "        self.drop_hr.end_of_sequence()\n",
    "        self.drop_hi.end_of_sequence()\n",
    "        \n",
    "    def forward(self, _input, prev_hidden):\n",
    "        c_i = self.drop_hr(prev_hidden)\n",
    "        h_i = self.drop_hi(prev_hidden)\n",
    "        \n",
    "        if self.is_first_layer:\n",
    "            x_i = self.drop_ii(_input)\n",
    "            x_r = self.drop_ir(_input)\n",
    "            h1 = nn.Tanh()(self.W_H(x_i) + self.R_H(h_i))\n",
    "            t1 = nn.Sigmoid()(self.W_C(x_r) + self.R_C(c_i))\n",
    "        else:\n",
    "            h1 = nn.Tanh()(self.R_H(h_i))\n",
    "            t1 = nn.Sigmoid()(self.R_C(c_i))\n",
    "            \n",
    "        h = (h1 * t1) + (prev_hidden * (1 - t1))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(AbstractGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias_ih = bias_ih\n",
    "        self.bias_hh = bias_hh\n",
    "        self.gru_cell = None\n",
    "        self._load_gru_cell()\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        # Interface\n",
    "        raise NotImplementError\n",
    "        \n",
    "    def forward(self, x, hx=None, max_length=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        if max_length is None:\n",
    "            max_length = seq_length\n",
    "        output = []\n",
    "        for i in range(max_length):\n",
    "            # hidden output of every time-step\n",
    "            hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            output.append(hx.view(batch_size, 1, self.hidden_size))\n",
    "        result = torch.cat(output, 1)\n",
    "        return result, hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(AbstractGRU):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(GRU, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        self.gru_cell = GRUCell(self.input_size, self.hidden_size, self.bias_ih, self.hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiBayesianGRU(AbstractGRU):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False, dropout=0.25):\n",
    "        self.dropout = dropout\n",
    "        super(BiBayesianGRU, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        self.gru_cell = BayesianGRUCell(self.input_size, self.hidden_size, self.bias_ih, self.bias_hh, dropout=self.dropout)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = dropout\n",
    "        self.gru_cell.set_dropout(dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(1, batch_size, self.hidden_size)).cuda()\n",
    "    \n",
    "    def forward(self, x, hx=None, max_length=None, lengths=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        if max_length is None:\n",
    "            max_length = seq_length\n",
    "        lefts, rights = [], []\n",
    "        \n",
    "        # left part\n",
    "        lhx = self.init_hidden(batch_size)\n",
    "        for i in range(max_length):\n",
    "            new_hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            mask = (i < lengths).float().unsqueeze(1).expand_as(new_hx)\n",
    "            lhx = new_hx * mask + lhx * (1 - mask)\n",
    "            lefts.append(lhx.view(batch_size, 1, self.hidden_size))\n",
    "        self.gru_cell.end_of_sequence()\n",
    "        lefts = torch.cat(lefts, 1)\n",
    "        \n",
    "        # right part\n",
    "        rhx = self.init_hidden(batch_size)\n",
    "        for i in range(max_length - 1, -1, -1):\n",
    "            new_hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            mask = (i < lengths).float().unsqueeze(1).expand_as(new_hx)\n",
    "            rhx = new_hx * mask + rhx * (1 - mask)\n",
    "            rights.append(rhx.view(batch_size, 1, self.hidden_size))\n",
    "        self.gru_cell.end_of_sequence()\n",
    "        rights = torch.cat(rights, 1)\n",
    "        \n",
    "        output = torch.cat((lefts, rights), dim=2)\n",
    "        return output, lhx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager(object):\n",
    "    \n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        \n",
    "    def save_model(self, model, path=None):\n",
    "        path = self.path if path is None else path\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(\"Model has been saved as %s.\\n\" % path)\n",
    "        \n",
    "    def load_model(self, model, path=None):\n",
    "        path = self.path if path is None else path\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        print(\"A pre-trained model at %s has been loaded.\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        print(\"Choose the torch base model.\")\n",
    "        self.manager = ModelManager()\n",
    "        \n",
    "    def save(self, path):\n",
    "        self.manager.save_model(self, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.manager.load_model(self, path)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, x, batch_size=256, verbose=0):\n",
    "        self.eval_model()\n",
    "        predictions = []\n",
    "        for batch_xs in test_batches_generator(x, batch_size):\n",
    "            preds_var = self.forward(batch_xs)\n",
    "            preds_logits = nn.Sigmoid()(preds_var)\n",
    "            predictions.append(preds_logits.data.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        return predictions\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        pass\n",
    "    \n",
    "    def train_model(self, p):\n",
    "        self.set_dropout(p)\n",
    "        self.train()\n",
    "        \n",
    "    def eval_model(self):\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=2, dropout=0.35, bidirectional=True)\n",
    "        \n",
    "        self.attn = DotAttention(hidden_size=2 * hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderDict([\n",
    "                ('gru_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 6, 108)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(108, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        _input, lengths = _input\n",
    "        embedded = self.embedding(_input)\n",
    "        \n",
    "        out, _ = self.gru(embedded)\n",
    "        last = out[:, -1, :]\n",
    "        attn, _ = self.attn.forward(out)\n",
    "        max_num, _ = torch.max(out, dim=1)\n",
    "        concatenated = torch.cat([last, max_num, attn], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGRUClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding):\n",
    "        super(BayesianGRUClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru1 = BiBayesianGRU(input_size=self.input_size, hidden_size=self.hidden_size, dropout=0.3)\n",
    "        self.gru2 = BiBayesianGRU(input_size=2 * self.hidden_size, hidden_size=hidden_size, dropout=0.3)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('gru_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 4, 72)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(72, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        self.gru1.set_dropout(p)\n",
    "        self.gru2.set_dropout(p)\n",
    "        \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        _input, lengths = _input\n",
    "        embedded = self.embedding(_input)\n",
    "        \n",
    "        out1, _ = self.gru1.forward(embedded, lengths=lengths)\n",
    "        out2, _ = self.gru2.forward(out1, lengths=lengths)\n",
    "        \n",
    "        last = out2[:, -1, :]\n",
    "        max_num, _ = torch.max(out2, dim=1)\n",
    "                \n",
    "        concatenated = torch.cat([last, max_num], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentHighwayClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, recurrent_length, embedding, recurrent_dropout=0.3):\n",
    "        super(RecurrentHighwayClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.L = recurrent_length\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.highways = nn.ModuleList()\n",
    "        self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=True, recurrent_dropout=self.recurrent_dropout))\n",
    "        \n",
    "        for _ in range(self.L - 1):\n",
    "            self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=False, recurrent_dropout=self.recurrent_dropout))\n",
    "            \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('h1_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 4, 74)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(74, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def init_state(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "        return hidden\n",
    "    \n",
    "    def set_dropout(self, p):\n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.set_dropout(p)\n",
    "            \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        '''\n",
    "        Input including input_sequences and sequence_length.\n",
    "        '''\n",
    "        _input, lengths = _input\n",
    "        batch_size = _input.size(0)\n",
    "        seq_length = _input.size(1)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_state(batch_size)\n",
    "        embed_batch = self.embedding(_input)\n",
    "        \n",
    "        lefts, rights = [], []\n",
    "        \n",
    "        for time in range(seq_length):\n",
    "            for tick in range(self.L):\n",
    "                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n",
    "                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n",
    "                hidden = next_hidden * mask + hidden * (1 - mask)\n",
    "            lefts.append(hidden.unsqueeze(1))\n",
    "        lefts = torch.cat(lefts, dim=1)\n",
    "        \n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.end_of_sequence()\n",
    "            \n",
    "        for time in range(seq_length - 1, -1, -1):\n",
    "            for tick in range(self.L):\n",
    "                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n",
    "                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n",
    "                hidden = next_hidden * mask + hidden * (1 - mask)\n",
    "            rights.append(hidden.unsqueeze(1))\n",
    "        rights = torch.cat(rights, dim=1)\n",
    "        \n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.end_of_sequence()\n",
    "            \n",
    "        outputs = torch.cat([lefts, rights], dim=2)\n",
    "        \n",
    "        last = outputs[:, -1, :]\n",
    "        max_num, _ = torch.max(outputs, dim=1)\n",
    "        \n",
    "        concatenated = torch.cat([last, max_num], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fasttext-avcnn-pos-' + str(nb_words) + 'vocabulary-' + str(MAX_SEQUENCE_LENGTH) + 'length'\n",
    "model = get_av_pos_cnn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(train_data, pos_train_data, train_labels, FOLD_COUNT, BATCH_SIZE, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall val-loss: {}, AUC {}'.format(val_loss, total_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgru_network():\n",
    "    embedding = nn.Embedding(MAX_NB_WORDS, EMBEDDING_DIM)\n",
    "    embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "    embedding.weight.requires_grad=False\n",
    "    return BayesianGRUClassifier(input_size=EMBEDDING_DIM, hidden_size=60, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrent_higtway_classifier():\n",
    "    embedding = nn.Embedding(MAX_NB_WORDS, EMBEDDING_DIM)\n",
    "    embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "    embedding.weight.requires_grad=False\n",
    "    return RecurrentHighwayClassifier(input_size=EMBEDDING_DIM, hidden_size=60, embedding=embedding, recurrent_length=2, recurrent_dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss, total_auc, fold_predictions = pytorch_train_folds(x=train_sequences, y=train_labels, fold_count=10, batch_size=256, get_model_func=get_recurrent_highway_classifier, skip_fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/CNN_Based/' + model_name\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_data_dict = {'Onehot': test_data, 'POS': pos_test_data}\n",
    "    test_predict = model.predict(test_data_dict, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    np.save('parameters_pool/AVPOSCNN/{}-AV-POS-CNN.npy'.format(fold_id), test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/rhn/Fasttext-rhn-' + str(MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_predict = model.predict(test_sequences, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOB (Out-of-Bag) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_predictions = np.concatenate((fold_predictions), axis=0)\n",
    "train_auc = roc_auc_score(train_labels, train_fold_predictions)\n",
    "print('Training AUC', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "train_ids = train_df['id'].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_predictions, columns=list_classes)\n",
    "train_predicts['id'] = train_ids\n",
    "train_predicts = train_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-(Train)-L{:4f}-A{:4f}.csv'.format(val_loss, train_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Estimation (For Train Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = train_df.iloc[:, :8]\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction = pd.read_csv('results/RNN_Based/fasttext-avrnn-100000vocabulary-350length-(Train)-L0.013377-A0.998050.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term_pos(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 1)\n",
    "    print('Wrong pos-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    pos = pd.DataFrame()\n",
    "    pos['id'] = train[diff]['id']\n",
    "    pos['text'] = train[diff]['comment_text']\n",
    "    pos['pred_val'] = pred[diff][check_column]\n",
    "    pos['label'] = train[diff][check_column]\n",
    "    return pos\n",
    "\n",
    "def get_error_term_neg(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 0)\n",
    "    print('Wrong neg-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    neg = pd.DataFrame()\n",
    "    neg['id'] = train[diff]['id']\n",
    "    neg['text'] = train[diff]['comment_text']\n",
    "    neg['pred_val'] = pred[diff][check_column]\n",
    "    neg['label'] = train[diff][check_column]\n",
    "    return neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in list_classes:\n",
    "    print('In term: ', term)\n",
    "    err_neg = get_error_term_neg(train_df, check_prediction, term)\n",
    "    err_pos = get_error_term_pos(train_df, check_prediction, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
