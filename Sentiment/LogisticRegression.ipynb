{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import re, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv').fillna(' ')\n",
    "test = pd.read_csv('datasets/test.csv').fillna(' ')\n",
    "\n",
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\t0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "\n",
      "severe_toxic\t0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "\n",
      "obscene\t0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "\n",
      "threat\t0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "\n",
      "insult\t0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "\n",
      "identity_hate\t0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    print(\"{}\\t{}\\n\".format(class_name, train[class_name].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(text):\n",
    "    replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Url cleaner\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    \n",
    "    # English & punctuation regularization\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : clean text process Done!\n"
     ]
    }
   ],
   "source": [
    "train_text = []\n",
    "test_text = []\n",
    "for text in list_sentences_train:\n",
    "    train_text.append(clean_word(text))\n",
    "\n",
    "for text in list_sentences_test:\n",
    "    test_text.append(clean_word(text))\n",
    "    \n",
    "all_text = np.concatenate([train_text, test_text])\n",
    "print(\"MSG : clean text process Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_sentences_train, list_sentences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : word vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "print(\"MSG : word vectorization process Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : char vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer=r'char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000\n",
    ")\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "print(\"MSG : char vectorization process Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release list memory\n",
    "del train_text, test_text, all_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_features = hstack([test_char_features, test_word_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_char_features, train_word_features, test_char_features, test_word_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9880999630952629\n",
      "...... Processing severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.9973888157757306\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9951745315535502\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997353999986074\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9920202210106327\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9984889947288894\n",
      "1 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9881557508320916\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9974654281615999\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9952790117955074\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997493280599384\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9919088668235687\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9986561198768922\n",
      "2 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9882741236926762\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9973957970671383\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9952511593577228\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997493280599384\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9920411659030457\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.998600415001323\n",
      "3 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9883646441154762\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9974445388332613\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9951188602782458\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997423649504923\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9921247232163996\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9985238207974153\n",
      "4 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9883507178965839\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9974375757238152\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9949726349798766\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997005862938154\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9921107969975073\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9986700460957846\n",
      "5 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9883228654587993\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9973609815199075\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9952859749049535\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997354018410461\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9916860473212918\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9985238207974153\n",
      "6 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9882114557076608\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9973679446293536\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9950422660743382\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997423649504923\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9918183464007687\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9984959683596307\n",
      "7 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9882253819265531\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.997611653459969\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9953625691088612\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997284387316\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9918879774952303\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9986143412202153\n",
      "8 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.98817664016043\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9974445388332613\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9950840447310151\n",
      "...... Processing threat\n",
      "Training accuracy is 0.999686660074923\n",
      "...... Processing insult\n",
      "Training accuracy is 0.991741752196861\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.9986352305485537\n",
      "9 fold:\n",
      "...... Processing toxic\n",
      "Training accuracy is 0.9880791566281839\n",
      "...... Processing severe_toxic\n",
      "Training accuracy is 0.9973122397537845\n",
      "...... Processing obscene\n",
      "Training accuracy is 0.9952999011238458\n",
      "...... Processing threat\n",
      "Training accuracy is 0.9997075494032616\n",
      "...... Processing insult\n",
      "Training accuracy is 0.9920481290124918\n",
      "...... Processing identity_hate\n",
      "Training accuracy is 0.998600415001323\n",
      "MSG : Done for k-fold cross validation!\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_features)):\n",
    "    print('{} fold:'.format(i))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    for class_name in class_names:\n",
    "        print('...... Processing {}'.format(class_name))\n",
    "        train_target = train[class_name][train_idx]\n",
    "        classifier.fit(train_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_features[train_idx])\n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "    models.append(classifier)\n",
    "print(\"MSG : Done for k-fold cross validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : Finished predict the proba for toxic\n",
      "MSG : Finished predict the proba for severe_toxic\n",
      "MSG : Finished predict the proba for obscene\n",
      "MSG : Finished predict the proba for threat\n",
      "MSG : Finished predict the proba for insult\n",
      "MSG : Finished predict the proba for identity_hate\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    print(\"MSG : Finished predict the proba for {}\".format(class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict(predictions)\n",
    "submission.to_csv('results/Logistic-Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9570610687741216\n",
      "CV score for class severe_toxic is 0.9422976332912251\n",
      "CV score for class obscene is 0.9804917156315561\n",
      "CV score for class threat is 0.852109145437934\n",
      "CV score for class insult is 0.9646194357605319\n",
      "CV score for class identity_hate is 0.9135748399233806\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
